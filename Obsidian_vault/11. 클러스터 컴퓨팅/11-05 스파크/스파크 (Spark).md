# 1-1. 스파크 등장
- ## 아파치 스파크 (Apache Spark)
	- 아파치 스파크는 대규모 데이터 **처리**를 하기위해 설계된 **통합형** 엔진
		- 통합 (unified) : 스파크는 간단한 데이터 읽기부터 SQL 처리, 머신러닝, 스트림 처리등 다양한 데이터 분석 작업을 같은 연산 엔진과 일관성 있는 API로 수행할수 있도록 설계되어 있음
	- 아파치 스파크는 Scala와 Java로 작성되어 있으며 JVM 기반 애플리케이션
		- Spark는 JVM 위에서 실행되며, JVM의 다양한 기능 활용 가능
		- Spark 애플리케이션을 실행할 때, 새로운 JVM 프로세스 시작됨
		- Spark executor 등 구성요소가 JVM내에서 실행됨
		![](https://i.imgur.com/9EfrYrc.png)
	- ### JVM(Java Virtual Machine)이란
		- Java 바이트 코드를 실행하는 가상 머신
		- Java 애플리케이션이 다양한 하드웨어 및 운영체제에서 동일하게 실행될 수 있도록 해줌
		- JVM은 Java 프로그램을 실행할 수 있는 런타임 환경 제공
			- (Java 프로그램을 실행하는데 필요한 환경 제공)
	- ### JVM의 기능
		- **바이트 코드 실행** : Java 소스코드를 실행, JVM이 설치된 모든 환경에서 실행
		- **메모리 관리** : 가비지 컬렉션(Garbage Collection) 기능을 통해 사용되지 않는 객체를 자동으로 메모리에서 해체하여 메모리 누수 방지
		- **플랫폼 독립성** : JVM은 운영체제 및 하드웨어에 종속되지 않고 동일한 바이트 코드 실행 가능
		- **보안** : JVM은 바이트 코드 검증, 클래스 로더, 보안 매니저 등을 통해 애플리케이션 보안 강화
	![600](https://i.imgur.com/MvBp5od.png)
	- ### RDD (Resilient Distributed Dataset)
		- 대부분의 경우 구조적 API를 사용하는 것이 좋음
			- 그러나 구조적 API로 처리가 안되는 경우에는 저수준 API를 사용해야할 수도 있음
			- RDD는 스파크 1.x 버전의 핵심 API이지만 스파크 2.x 버전부터는 잘 사용 안함
		- 예를 들어,
			- 구조적 API에서 제공하지 않는 기능이 필요한 경우
				- (클러스터의 물리적 데이터의 배치를 세밀하게 제어)
			- RDD를 사용해 개발된 기존 코드를 유지해야하는 경우
		- 정말 필요한 경우가 아니라면 수동으로 RDD 생성 비추천
		- #### 장점
			- 모든 레코드는 자바나 파이썬 객체이므로 완벽하게 제어 가능
			- 사용자가 원하는 포맷을 사용해 원하는 모든 데이터 저장 가능
		- #### 단점
			- 개발자는 강력한 제어권을 가질수 있지만 잠재적 문제 발생 가능
			- 모든 값을 다루거나 값 사이의 상호작용 과정을 반드시 수동으로 정의해야함
			- 구조적 API와 다르게 레코드의 내부구조를 스파크에서 파악할수 없으므로 최적화 하려면 훨씬 많은 수작업 요구
				- 구조적 API의 경우 자동으로 데이터를 포맷 타입을 구현해 모든 저수준 연산 과정에서 사용해야함
				- 스파크 SQL에서 자동으로 수행되는 필터 재정렬과 집계 같은 최적화 기법도 직접 구현해야함
	- ### 스파크 설계 철학 네 가지 특성
		- **속도** - 중간 결과를 메모리에 저장하여 디스크 I/O를 제한적으로 사용하므로 성능이 크게 향상됨
		- **사용 편리성** - RDD라는 단순한 논리 자료구조 사용
		- **모듈성** - 스파크 이외의 별도 엔진을 돌릴 필요 없음
		- **확장성**
	- ### 컴퓨팅 엔진
		- 스파크는 저장보다는 **빠른 병렬 연산 엔진**에 초점
		- 스파크는 저장소 시스템의 데이터를 연산하는 역할만 수행할 뿐 영구 저장소 역할은 수행하지 않음
			- 애저 스토리지(Azure Storage), 아마존 S3, 하둡, 카산드라, 카프카 등의 저장소 연동 지원
		- 스파크는 특정 저장소 시스템을 선호하지 않음
		- 스파크는 데이터 저장 위치에 상관없이 처리에 집중하도록 만들어짐
	- ### 하둡과의 차이점
		- 하둡은 저장과 연산을 모두 포함
		- 하둡과 같은 구조에서는 저장 혹은 연산 둘중 하나의 시스템만 단독으로 사용하기 어려움
		- 하둡은 다른 저장소의 데이터에 접근하는 애플리케이션을 개발하기 어려움
		- 스파크는 하둡 저장소와도 잘 호환됨
	- ### 스파크 SQL
		- 구조화된 데이터를 다룰때 유용함
		- REBMS 테이블이나 구조화된 데이터의 파일 포맷에서 데이터를 읽어들일 수 있음
			- CSV, TXT, JSON, Parquet
			![500](https://i.imgur.com/lFa0XmG.png)
			