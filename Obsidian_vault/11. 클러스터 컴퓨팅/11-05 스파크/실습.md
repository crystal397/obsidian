- # pyenv
	- ## net-tools 설치
		`sudo apt-get update`
		`sudo apt-get install net-tools`
	- ## pyenv 설치
		```
		sudo apt-get update; sudo apt-get install make build-essential libssl-dev zlib1g-dev \
		libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \
		libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev
		```
		`curl https://pyenv.run | bash`
		- ### pyenv 환경 설정
			```
			vim .bashrc
			
			# 가장 아래에 추가
			export PYENV_ROOT="$HOME/.pyenv"
			[[ -d $PYENV_ROOT/bin ]] && export PATH="$PYENV_ROOT/bin:$PATH"
			eval "$(pyenv init -)"
			eval "$(pyenv virtualenv-init -)"
			```
			- #### 쉘 재시작
				`exec $SHELL`
			- #### 설치 확인
				`pyenv`
			- #### 가상 환경 생성
				`pyenv install --list`
				`pyenv install 3.11.9`
			- #### 가상 환경 생성
				`pyenv versions`
				`pyenv virtualevn 3.11.9 py3_11_9`
				`pyenv activate py3_11_9`
				`pyenv deactivate`
			- #### 필요한 라이브러리 설치
				`pyenv activate py3_11_9`
				`pip install numpy`
				`pip install pandas`
				`pip install pyarrow`
				`pip install scikit-learn`
				`pip install flask`
				`pip install gunicorn`
				`pip install psycopg2-binary`
				`pyenv deactivate`
- # PySpark 라이브러리 설치
	- ## Java 설치
		![400](https://i.imgur.com/B8yIit8.png)
		`sudo apt install openjdk-17-jre-headless`
		![400](https://i.imgur.com/6YIcPH3.png)
		`java -version`
	- ## Java 환경 변수 추가
		`ls /usr/lib/jvm/`
		`vim .bashrc`
		`export JAVA_HOME=/usr/lib/jvm/java-1.17.0-openjdk-amd64`
		`source .bashrc`
		`exec $SHELL`
		`$JAVA_HOME`
	- ## pyspark 설치
		`pyenv activate py3_11_9`
		`pip install pyspark`
		`pyenv deactivate`
- #  csv -> parquet 변환
	- ## 실습 디렉토리 생성
		```
		mkdir work
		cd work
		mkdir spark
		cd spark
		
		/home/ubuntu/work/spark
		```
	- ## pandas로 csv 파일 불러오기
		```
		pyenv activate py3_11_9
		python
		>>>> import pandas as pd
		>>>> url = 'https://raw.githubusercontent.com/losskatsu/data-example/main/data/iris.csv'
		>>>> df = pd.read_csv(url)
		>>>> df.head(3)
		```
		![400](https://i.imgur.com/x1pItRJ.png)
	- ## pandas dataframe -> csv, parquet 파일 변환
		```
		>>>> df.to_parquet('/home/ubuntu/work/spark/iris.parquet', index=False)
		>>>> df.to_csv('/home/ubuntu/work/spark/iris.csv', index=False)
		>>>> quit()
		ls
		```
		![300](https://i.imgur.com/Xv1rumC.png)
		- csv 파일 및 parquet 파일 생성 확인
- # pyspark 기초
	- ## csv 파일 불러오기
		```
		python
		>>>> from pyspark.sql import SparkSession
		>>>> spark = SparkSession.builder.appName("CSVReader").getOrCreate()
		```
		![](https://i.imgur.com/XxzurvD.png)
		- 하둡이 없다는 경고, 하둡 안쓸것이므로 무시
		- ### pyspark.sql
			- 데이터프레임(DataFrame)과 SQL을 사용해 데이터를 조작, 분석하는 기능 제공
			- #### SparkSession
				- Spark에서 사용되는 진입점(entry point)
				- Spark 애플리케이션과 클러스터 간의 연결을 담당
				- SparkSession 을 활용하면 Spark SQL, Dataframe, RDD(Resilient Distributed Dataset) API 사용 가능
				- 다양한 데이터 소스(JDBC, Hive, Parquet, JSON 등)에 연결하여 데이터 조작 가능
				- 실행 환경 설정
			- `spark = SparkSession.builder.appName("CSVReader").getOrCreate()`
				- builder : SparkSession을 생성하기 위한 빌더 객체 변환
				- appName : Spark 애플리케이션의 이름 지정
				- getOrCreate : 객체를 생성하거나 기존 객체를 반환하는 메서드
		```
		>>>> df = spark.read.option("header","true").csv("/home/ubuntu/work/spark/iris.csv")
		>>>> df.show(5)
		```
		![300](https://i.imgur.com/GOKszaE.png)
	- ## parquet 파일 불러오기
		```
		python
		>>>> from pyspark.sql import SparkSession
		>>>> spark = SparkSession.builder.appName("ParquetReader").getOrCreate()
		>>>> df = spark.read.parquet("/home/ubuntu/work/spark/iris.parquet")
		>>>> df.show(8)
		```
		![300](https://i.imgur.com/adVtpYp.png)
		- show : DataFrame 데이터를 콘솔로 출력할때 사용
	- ## 데이터스키마 확인
		```
		>>> df.printSchema()  
		>>> type(df)
		```
	- ## 전체 데이터 행(row) 개수 & 열(column) 개수 확인
		```
		>>> df.count()
		>>> df.columns
		>>> len(df.columns)
		```
		![300](https://i.imgur.com/soEVnKp.png)
	- ## 특정 행 출력
		```
		>>> df.collect()[3]
		>>> df.collect()[3:6]
		```
		![400](https://i.imgur.com/OZV6Vif.png)
		- ### collect 함수
			- DataFrame의 모든 데이터를 드라이버 프로그램으로 수집하여 반환하는 함수
			- 클러스터 내의 모든 노드에서 데이터를 모아 로콜 머신의 메모리로 가져오는데 사용
			- 따라서 너무 큰 데이터셋에 대해 사용하면 메모리 부족 문제 발생할 수도 있음
	- ## 특정 열 출력
		```
		>>> df.select('sepal_length','petal_width').show(3)
		```
		![300](https://i.imgur.com/kwV1uL6.png)
		- 데이터 프레임의 특정 열을 선택 하는데 사용
		- SQL의 SELECT 문과 유사하게 동작
		```>>> cols01 = df.select('sepal_length','petal_width')
		>>> cols01.show(5)
		```
		![300](https://i.imgur.com/KpzXNuH.png)
- ### day 02
- #### 복습
	```
	# ec2 접속
	# pyenv 실행
	pyenv activate py3_11_9
	
	# python 실행
	$ python
	
	# parquet 파일 불러오기
	>>> from pyspark.sql import SparkSession
	>>> spark = SparkSession.builder.appName("ParquetReader").getOrCreate()
	>>> df = spark.read.parquet("/home/ubuntu/work/spark/iris.parquet")
	>>> df.show(7)
	
	# 데이터스키마 확인
	>>> df.printSchema()
	
	# 전체 데이터 행(row) 개수 & 열(column) 개수 확인
	>>> df.count()
	>>> df.columns
	>>> len(df.columns)
	
	# 특정 행 출력
	>>> df.collect()[3]
	>>> df.collect()[3:6]
	
	# 특정 열 출력
	>>> df.select('sepal_length','sepal_width').show(3)
	>>> cols = df.select('sepal_length','sepal_width')
	>>> cols.show(3)
	```
	- ## 기초통계량 출력
		`df.describe().show()`
		![600](https://i.imgur.com/4RbPOt6.png)
		- stddev : 표준편차
		- 평균(location parameter) -> 데이터의 위치를 알수 있다
		- 표준편차(scale parameter) -> 데이터의 흩어짐 정도
	- ## filter 기능
		`>>> df.filter("sepal_length > 7").show(3)`
		![300](https://i.imgur.com/GFYlcbK.png)
		- filter : 조건에 맞는 행들만 선택
	- ## filter & select
		`>>> df.filter("sepal_length > 7").select("sepal_length", "petal_length").show(10)`
		![400](https://i.imgur.com/OD39cU8.png)
		- sepal_length가 7보다 큰 데이터 중 sepal_length, petal_length 열 만 보여줌
	- ## group by
		`>>> df.groupby("class").agg(avg("sepal_length")).show()`
		`>>> mean_sl = df.groupby("class").agg(avg("sepal_length"))`
		`>>> mean_sl.show()`
		- 클래스 별로 묶어서 클래스 별로 평균
		![300](https://i.imgur.com/8qGdWYZ.png)
		![300](https://i.imgur.com/XJM6YvN.png)
	- ## order by(오름차순)
		`df.orderBy("petal_length", ascending=True).show(5)`
		![300](https://i.imgur.com/OYg8Esq.png)
	- ## order by(내림차순)
		`df.orderBy("petal_length", ascending=True).show(5)`
		![300](https://i.imgur.com/R9BfviY.png)
		```
		>>> quit()
		$ pyenv deactivate
		```
- # Spark
	![400](https://i.imgur.com/ILeUA0E.png)
	- 지금까지는 스파크 라이브러리 를 설치하고 사용함
	- 주피터 랩을 사용해서 parquet 파일 불러오기
	- ### 디렉토리 정리
		```
		$ cd work
		
		/work$ mv spark spark01
		
		/work/spark01$ mkdir data
		
		/work/spark01$ mv iris.csv iris.parquet data
		
		/work/spark01$ cd data
		
		/work/spark01/data$ ls
		iris.csv  iris.parquet
		
		/work/spark01/data$ pwd
		/home/ubuntu/work/spark01/data
		
		cd 
		$ mkdir app
		$ cd app
		/app$ mkdir spark
		/app$ cd spark
		```
	- ## spark download
		- https://www.apache.org/dyn/closer.lua/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
			![400](https://i.imgur.com/uOVKsIc.png)
		`/app/spark$ wget https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz`
	- ## 압축 풀기
		`tar -zxvf spar-3.5.1-bin-hadoop3.tgz`
	- ## .bashrc 설정 추가
		```
		vim .bashrc
		
		export SPARK_HOME=/home/ubuntu/app/spark/spark-3.5.1-bin-hadoop3
		export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
		export PYSPARK_PYTHON=/home/ubuntu/.pyenv/versions/py3_11_9/bin/python
		
		source .bashrc
		exec $SHELL
		```
		`/app/spark/spark-3.5.1-bin-hadoop3$ ls`
		![](https://i.imgur.com/NWMBFUJ.png)
		- bin : 스파크와 상호작용할 수 있는 스크립트로 구성
		- kubernetes : 쿠버네티스 클러스터에서 쓰는 스파크를 위한 도커 이미지 제작을 위한 Dokerfile 포함
		- python : pyspark과 관련된 파이썬 파일 및 라이브러리 포함 (스파크를 실행할 때는 로컬에 설치된 파이썬 사용)
		- sbin : 스파크 컴포넌트 시작 및 중지, 관리
		`/app/spark/spark-3.5.1-bin-hadoop3/bin$ ls`
		 ![600](https://i.imgur.com/bggHoi2.png)
		 - pyspark : 파이썬으로 실행
		 - spark-shell : 스칼라로 실행
	 - ## spark 실행
		 `./pyspark`
		 ![600](https://i.imgur.com/mUrulB7.png)
		 ```
		 # 4040 포트로 web ui 확인 가능
		 http://ip-172-31-7-51.ap-northeast-2.compute.internal:4040
		 ```
		 - {ec2 퍼블릭 IP}:4040
		 ![400](https://i.imgur.com/NeBGSwR.png)
		```
		 # 스파크 세션이 이미 만들어져 있음
		 `SparkSession available as 'spark'.`
		```
		`df = spark.read.parquet("/home/ubuntu/work/spark01/data/iris.parquet")`
		`df.show(7)`
		![300](https://i.imgur.com/ed6ULUO.png)
	- ## spark web ui 확인
		- 4040포트 열어주기
		![](https://i.imgur.com/8fPcaL1.png)
		![](https://i.imgur.com/ic9GgAn.png)
		- ### spark 종료
			`>>> quit()`
- # Jupyter Lab 설치
	- ## Jupyter Lab 디렉토리 설정
		```
		cd
		cd work
		mkdir jupyter
		cd jupyter
		pwd
		```
	- ## Jupyter Lab 설치
		```
		pyenv activate py3_11_9
		pip install hupyterlab
		```
	- ## Jupyter Lab 설정
		`jupyter lab --generate-config`
		![400](https://i.imgur.com/3gfqmXO.png)
		![400](https://i.imgur.com/2yrgxg4.png)
		`vim jupyter_lab_config.py`
		```
		# 모든 네트워크 요청 수신
		c.NotebookApp.ip = '0.0.0.0' 
		# 자동으로 웹 브라우저를 열지 않음
		c.NotebookApp.open_browser = False 
		# 주피터 랩 서버가 사용할 포트
		c.NotebookApp.port = 8888 
		# 주피터 랩 서버에 접속할 때 사용할 인증 토큰, 빈 문자열로 설정하면 토큰 인증 비활성화
		c.NotebookApp.token = '' 
		# 주피터 랩 서버에 접속할 때 사용할 인증 비밀번호, 빈 문자열로 설정하면 비밀번호 비활성화
		c.NotebookApp.password = ''
		# 주피터 랩의 루트 디렉토리
		c.ServerApp.root_dir = '/home/ubuntu/work/jupyter'
		```
		![300](https://i.imgur.com/C97qwDo.png)
	- ## .bashrc 설정 추가
		```
		cd
		vim .bashrc
		
		export PYSPARK_DRIVER_PYTHON="jupyter"
		export PYSPARK_DRIVER_PYTHON_OPTS="lab"
		```
		- ### 셀 재시작
			`pyenv deactivate`
			`source .bashrc`
			`exec $SHELL`
	- ## findspark 설치
		```
		pyenv activate py3_11_9
		pip install findspark
		```
	- ## AWS 보안 설정
		![600](https://i.imgur.com/7oVC2ZH.png)
		- 8888포트 열어주기
- # Jupyter Lab 실행
	- ## Jupyter Lab 실행
		`$ jupyter lab`
		{ec2 퍼블릭 IP}:8888
		![300](https://i.imgur.com/ODxEfHY.png)
		- Notebook -> Python 3 클릭
		![300](https://i.imgur.com/klb4qIF.png)
		- ### 단축기 +
			- rename
			![200](https://i.imgur.com/seeSvAb.png)
			- ctrl + s  누르면 저장(로그 출력됨)
				![400](https://i.imgur.com/87XzHji.png)
			- shift + s 라인 실행
			- 위 라인 추가, 아래 라인 추가, 삭제
				![100](https://i.imgur.com/h7lvMdS.png)
			- a : 위 추가
			- b : 아래 추가
			- dd : 삭제
			- z : 이전으로
			- 커널 초기화
				![300](https://i.imgur.com/OGdNVP7.png)
			- 커널 초기화 + 전부 실행
				![200](https://i.imgur.com/V8DDebn.png)
			- 마크다운
				![200](https://i.imgur.com/E898lhW.png)
				- 작성 후 shift + Enter
				- 수정 은 더블클릭
				- 예시
					```md
					# 월간 보고서
					## 주간 보고서
					### 마크다운 문법(markdown)
					* 사과
					* 배
					* 포도
					이름 | 소속 | 취미
					-----|-----|-----
					문기오 | 이어드림 4기 | 게임
					강수빈 | 이어드림 4기 | 코딩
					```
					![300](https://i.imgur.com/UhELjtD.png)
	- ### 실습
		`import findspark`
		`findspark.init("/home/ubuntu/app/spark/spark-3.5.1-bin-hadoop3")`
		- ↑ 주피터 랩 과 스파크 를 붙임
		`spark = SparkSession.builder.appName("test").getOrCreate()`
		`df = spark.read.parquet("/home/ubuntu/work/spark01/data/iris.parquet")`
		`df.show(3)`
			![300](https://i.imgur.com/EUogNd5.png)
	- 주피터 노트북 확장자로 .ipynb 많이 사용함
- # Spark cluster 구축
	![400](https://i.imgur.com/mfTFQNX.png)
	- 서버 1을 마스터로
	- 나머지는 worker
	- ## 두 번째, 세번째 인스턴스 생성
		- 인스턴스 이름 : 이름02, 이름03
		![200](https://i.imgur.com/X9ssmio.png)
		- ubuntu 선택
		![200](https://i.imgur.com/jMBvntH.png)
		- t2.xlage 선택
		![200](https://i.imgur.com/mVPWTnd.png)
		- 01 과 같은 보안그룹 선택
		![200](https://i.imgur.com/AUyaASc.png)
		- 스토리지 구성 100GiB
		- 인스턴스 시작
		- ### 01 ~ 03 전부 접속 확인
			`ssh -i ~/.ssh/mungio.pem ubuntu@{퍼블릭 IP}`
			`df -h` -> 용량 확인
- # 인스턴스 간 트래픽 허용
	- ## 인스턴스간 ping 가능하게 설정
		![200](https://i.imgur.com/Eup4PHG.png)
		- 같은 보안그룹으로 만든지 확인!!
		![500](https://i.imgur.com/oTzBl9T.png)
		- 규칙 추가 -> 모든 트래픽 -> 본인 사용한 보안그룹 검색해서 선택
	- ## net-tools 설치
		- 02, 03 둘다 실행
			`sudo apt update`
			`sudo apt install net-tools`
			- 01은 이미 설치 되어 있음
	- ## ip 주소 확인
		`ifconfig`
		`sudo vim /etc/hosts`
		- 3개 전부 작성 해주기
		```
		{IP 주소} {호스트 이름}
		172.31.7.51 ip-172-31-7-51
		172.31.3.190 ip-172-31-3-190
		172.31.6.106 ip-172-31-6-106
		```
- # ssh 인증키 생성
	- ssh란 ?
		- Secure Shell Protocol
		- 네트워크 프로토콜 중 하나
		- Public Network를 통해 서로 통신을 할 때 보안적으로 안전하게 통신을 하기 위해 사용하는 프로토콜이다
	- ## ssh-keygen -t rsa
		- RSA 알고리즘을 사용해 SSH 키 쌍을 생성하는 명령어
		- 명령어 입력 후에는 해당 경로에 다음과 같은 두 개의 파일이 생성됨
			- id_rsa : 개인 키(private key) 파일
			- id_rsa.pub : 공개 키(public key) 파일
	- ## ssh 인증키 생성
		- 서버 3개 전부 실행
		![400](https://i.imgur.com/jUP5cL0.png)
		- 명령어 입력후 Enter 3번 입력
	- ## ssh 인증키 생성 확인
		![400](https://i.imgur.com/275tn4S.png)
		![400](https://i.imgur.com/QWtwCvq.png)
		![400](https://i.imgur.com/qDN7byp.png)
	- ## rsa 공개키 확인
		`sudo cat .ssh/id_rsa.pub`
	- ## server01 - server02, 03의 공개키 추가
		- ### server02 - server01, 03
		- ### server03 -server 01, 02
		`sudo vim .ssh/authorized_keys`
		![600](https://i.imgur.com/XlMfYLY.png)
		- (1번 서버에 명령어 입력하면 본인의 키가 있음, 2번 3번만 추가 3번까지 본인 제외 나머지 반복)
- # 접속 확인
	- ## server01 -> server02, 03 접속 테스트
		`ssh {호스트 명}` -> `yes` (나올때 `exit`)
		![300](https://i.imgur.com/jX0II0n.png)
- # 클러스터 구축 준비
	- ## Java 설치 (server02, server 03)
		`sudo apt install openjdk-17-jre-headless`
		`java -version`
		`ls /usr/lib/jvm`
		`vim .bashrc`
		`export JAVA_HOME=/usr/lib/jvm/java-1.17.0-openjdk-amd64`
		`source .bashrc`
		`exec $SHELL`
	- ## pyenv 설치 및 가상환경 생성 (server02, server03)
		```
		sudo apt-get update; sudo apt-get install make build-essential libssl-dev zlib1g-dev \
		libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \
		libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev
		```
		`curl https://pyenv.run | bash`
		`vim .bashrc`
		```
		export PYENV_ROOT="$HOME/.pyenv"
		[[ -d $PYENV_ROOT/bin ]] &&
		export PATH="$PYENV_ROOT/bin:$PATH"
		eval "$(pyenv init -)"
		eval "$(pyenv virtualenv-init -)"
		```
		`exec $SHELL`
		`pyenv install 3.11.9`
		`pyenv virtualenv 3.11.9 py3_11_9`
		`pyenv versions`
		`pyenv activate py3_11_9`
		`pyenv deactivate`
	- ## 디렉토리 생성 (server02, server03)
		- server01
			`ls`
			![150](https://i.imgur.com/nGlyZOe.png)
		- server02, server03
			`ls`
			`mkdir app`
			`mkdir work`
			`ls`
			`cd app`
			`mkdir spark`
			`ls`
			`cd spark`
			![200](https://i.imgur.com/tWPFUW3.png)
	- ## spark 다운로드 (server02, server03)
		- apache spark download 검색
			![200](https://i.imgur.com/d9oe2Um.png)
			![300](https://i.imgur.com/OfAeVX7.png)
			![300](https://i.imgur.com/sXdER0V.png)
		- wget 으로 다운로드
			`wget https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz`
			`ls`
			![300](https://i.imgur.com/XKjtyoh.png)
	- ## spark 압축 풀기(server02, server03)
		`tar -zxvf spark-3.5.1-bin-hadoop3.tgz`
		![400](https://i.imgur.com/EcHg6kM.png)
		`ls`
		![300](https://i.imgur.com/a5CBT4b.png)
	- ## spark 설정 (server02, server03)
		`vim .bashrc`
		```
		export SPARK_HOME=/home/ubuntu/app/spark/spark-3.5.1-bin-hadoop3
		export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
		export PYSPARK_PYTHON=/home/ubuntu/.pyenv/versions/py3_11_9/bin/python
		```
		`source .bashrc`
		`exec $SHELL`
	- ## spark 실행 확인 (server02, server03)
		`pyspark`
		`>>> quit()`
		![600](https://i.imgur.com/Iy9Gaod.png)
- # 스파크 클러스터 설정
	- ## 마스터 노드 설정 (server01)
		`cd app/spark/spark-3.5.1-bin-hadoop3`
		`ls`
		`cd conf`
		`ls`
		`cp spark-env.sh.template spark-env.sh`
		![](https://i.imgur.com/euZPjeo.png)
		`vim spark-env.sh`
		`SPARK_MASTER_HOST='{server01 프라이빗 IP}'`
		`cp workers.template workers`
		![400](https://i.imgur.com/JG5gRbf.png)
		`vim workers`  ![130](https://i.imgur.com/6ySEsSa.png)
		```
		#localhost
		{server02 프라이빗 IP}
		{server03 프라이빗 IP}
		```
		![70](https://i.imgur.com/sNndBE2.png)
	- ## 워커 노드 설정 (server02, server03)
		`cd app/spark/spark-3.5.1-bin-hadoop3/conf`
		`cp spark-env.sh.template spark-env.sh`
		`vim spark-env.sh`
		`SPARK_MASTER_HOST='{server01 프라이빗 IP}'`
- # 스파크 클러스터 가동
	- ## 스파크 클러스터 가동 (server01)
		`~/app/spark/spark-3.5.1-bin-hadoop3$ ./sbin/start-all.sh`
		- (보안그룹 8080포트 열어두기)
	- ## 스파크 클러스터 가동 확인 (server01)
		- {server01 퍼블릭 IP}:8080 접속 확인
			![400](https://i.imgur.com/M0z9nFz.png)
		- ### server01, 02, 03
			`ps -ef | grep spark`
			![550](https://i.imgur.com/IeP8i1i.png)
	- ## jupyter lab 설정 (server01)
		`cd`
		`pyenv activate py3_11_9`
		`jupyter kernelspec list`
		`pyenv deactivate`
		![500](https://i.imgur.com/VqnBweJ.png)
		`cd /home/ubuntu/.pyenv/versions/3.11.9/envs/py3_11_9/share/jupyter/kernels`
		`ls`
		`mkdir py3spark`
		`cd py3spark`
		![500](https://i.imgur.com/tmXSFfP.png)
		`vi kernel.json` -> 기존에 없는 파일을 새라 만듬
		```
		{
			"argv": [
				"python",
				"-m",
				"ipykernel_launcher",
				"-f",
				"{connection_file}"
			],
			"display_name": "py3spark",
			"language": "python",
			"env":{
		        	"SPARK_HOME": "/home/ubuntu/app/spark/spark-3.5.1-bin-hadoop3",
		        	"PYSPARK_PYTHON": "/home/ubuntu/.pyenv/versions/py3_11_9/bin/python",
		        	"PYTHONPATH": "/home/ubuntu/app/spark/spark-3.5.1-bin-hadoop3/python:/home/ubuntu/app/spark/spark-3.5.1-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip"
		      }
		}
		```
	- ## jupyter lab 실행 (server01)
		`pyenv activate py3_11_9`
		`jupyter lab`
		{server01 퍼블릭 IP}:8888 접속 확인
		![](https://i.imgur.com/K6rPOMO.png)
		- py3spark 커널 생긴 것 확인
		- py3spark 클릭
	- ## 데이터 불러오기
		```
		import pyspark
		from pyspark.sql import SparkSession
		```
		`spark = SparkSession.builder.appName("test").getOrCreate()`
		`df = spark.read.parquet("/home/ubuntu/work/spark01/data/iris.parquet")`
		`df.show(10)`
		![300](https://i.imgur.com/k6BZv6n.png)
	- ## 데이터 요약
		`df.printSchema()`
		`df.describe().show()`
		![400](https://i.imgur.com/foOYJnF.png)
		`df.summary().show()`
		`df.columns`
		`df.count()`
		![400](https://i.imgur.com/Awwshdj.png)
		`df.distinct().show()`
		`df.distinct().count()`
		![250](https://i.imgur.com/HQoGbW8.png)
	- ## 데이터 필터링
		`df.select("sepal_length", "petal_length").show(5)`
		`df.filter(df["sepal_length"] > 6.0).show(7)`
		![250](https://i.imgur.com/9GOyE6k.png)
	- ## 데이터 요약
		`df.where(df["class"] == 2).show(6)`
		![250](https://i.imgur.com/3mPSZHz.png)
	- ## 데이터 추가 및 변환
		- ### 새로운 열 추가
		```
		from pyspark.sql.functions import col
		df = df.withColumn("sepal_length_cm", col("sepal_length") * 10)
		```
		`df.show(5)`
		![300](https://i.imgur.com/e17lpTX.png)
		- ### class가 0이면.. 1이면.. 2면..
		```
		from pyspark.sql.functions import when
		
		df = df.withColumn(
		    "class_string",
		    when(df["class"] == 0, "setosa")
		    .when(df["class"] == 1, "Versicolour")
		    .when(df["class"] == 2, "Virginica")
		)
		```
		`df.show(10)`
		![300](https://i.imgur.com/I84Akva.png)
		```
		from pyspark.sql.functions import substring
		df = df.withColumn("class_string3", substring(df["class_string"], 1, 3))
		
		```
		`df.show(5)`
		![](https://i.imgur.com/iJx1R27.png)
		![200](https://i.imgur.com/oTobJNI.png)
	- ## 그룹별 집계
		`df.groupBy("class").agg({"sepal_length": "avg", "sepal_width": "avg"}).show()`
		![400](https://i.imgur.com/GYGAnvJ.png)
	- ## 필터링
		`df.filter(df["sepal_length"] > 6).show(10)`
		![500](https://i.imgur.com/lRfN2Mo.png)
	- ## 데이터 시각화
		- ### server01 에서 matplotlib 설치
			```
			pyenv activate py3_11_9
			pip install matplotlib
			```
		```
		#PySpark DataFrame을 Pandas DataFrame으로 변환
		pandas_df = df.toPandas()
		
		# Matplotlib을 이용한 시각화
		import matplotlib.pyplot as plt
		pandas_df.hist(bins=50, figsize=(20,15))
		plt.show()
		```
		![500](https://i.imgur.com/dVOc2u6.png)
- # RDD
	`import pyspark`
	`from pyspark.sql import SparkSession`
	`spark = SparkSession.builder.appName("test").getOrCreate()`
	`test_sp = spark.range(10).rdd`
	`test_sp.collect()`
	![100](https://i.imgur.com/jT4xHiL.png)
	`test_sp.take(10)`
	![100](https://i.imgur.com/t94ariq.png)
	`test_df = spark.range(10).rdd.toDF()`
	`test_df.show(5)`
	![200](https://i.imgur.com/N93HHvy.png)
	`df = spark.read.parquet("/home/ubuntu/work/spark01/data/iris.parquet")`
	`df.show(10)`
	![300](https://i.imgur.com/9bZS8zN.png)
	- ## dataframe -> RDD
		`df_rdd = df.rdd`
		`df_rdd.take(10)`
		![400](https://i.imgur.com/07VNVYo.png)
	- ## RDD filter
		`df_rdd = df.rdd`
		`df_rdd.take(10)`
		`class_rdd = df_rdd.map(lambda row: row['class'])`
		`class_rdd.distinct()`
		`class_rdd.distinct().collect()`
		`class_rdd.distinct().count()`
		![400](https://i.imgur.com/zGexp30.png)
- # 스파크 클러스터 정지
	- ## server01
		`~/app/spark/spark-3.5.1-bin-hadoop3/sbin$ ./stop-all.sh`
		`pyenv deactivate`
		![](https://i.imgur.com/k59XRMN.png)
	- ## server01, 02, 03
		`ps -ef | grep spark`
		![](https://i.imgur.com/IiZYAiF.png)