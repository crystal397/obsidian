- # pyenv
	- ## net-tools 설치
		`sudo apt-get update`
		`sudo apt-get install net-tools`
	- ## pyenv 설치
		```
		sudo apt-get update; sudo apt-get install make build-essential libssl-dev zlib1g-dev \
		libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \
		libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev
		```
		`curl https://pyenv.run | bash`
		- ### pyenv 환경 설정
			```
			vim .bashrc
			
			# 가장 아래에 추가
			export PYENV_ROOT="$HOME/.pyenv"
			[[ -d $PYENV_ROOT/bin ]] && export PATH="$PYENV_ROOT/bin:$PATH"
			eval "$(pyenv init -)"
			eval "$(pyenv virtualenv-init -)"
			```
			- #### 쉘 재시작
				`exec $SHELL`
			- #### 설치 확인
				`pyenv`
			- #### 가상 환경 생성
				`pyenv install --list`
				`pyenv install 3.11.9`
			- #### 가상 환경 생성
				`pyenv versions`
				`pyenv virtualevn 3.11.9 py3_11_9`
				`pyenv activate py3_11_9`
				`pyenv deactivate`
			- #### 필요한 라이브러리 설치
				`pyenv activate py3_11_9`
				`pip install numpy`
				`pip install pandas`
				`pip install pyarrow`
				`pip install scikit-learn`
				`pip install flask`
				`pip install gunicorn`
				`pip install psycopg2-binary`
				`pyenv deactivate`
- # PySpark 라이브러리 설치
	- ## Java 설치
		![400](https://i.imgur.com/B8yIit8.png)
		`sudo apt install openjdk-17-jre-headless`
		![400](https://i.imgur.com/6YIcPH3.png)
		`java -version`
	- ## Java 환경 변수 추가
		`ls /usr/lib/jvm/`
		`vim .bashrc`
		`export JAVA_HOME=/usr/lib/jvm/java-1.17.0-openjdk-amd64`
		`source .bashrc`
		`exec $SHELL`
		`$JAVA_HOME`
	- ## pyspark 설치
		`pyenv activate py3_11_9`
		`pip install pyspark`
		`pyenv deactivate`
- #  csv -> parquet 변환
	- ## 실습 디렉토리 생성
		```
		mkdir work
		cd work
		mkdir spark
		cd spark
		
		/home/ubuntu/work/spark
		```
	- ## pandas로 csv 파일 불러오기
		```
		pyenv activate py3_11_9
		python
		>>>> import pandas as pd
		>>>> url = 'https://raw.githubusercontent.com/losskatsu/data-example/main/data/iris.csv'
		>>>> df = pd.read_csv(url)
		>>>> df.head(3)
		```
		![400](https://i.imgur.com/x1pItRJ.png)
	- ## pandas dataframe -> csv, parquet 파일 변환
		```
		>>>> df.to_parquet('/home/ubuntu/work/spark/iris.parquet', index=False)
		>>>> df.to_csv('/home/ubuntu/work/spark/iris.csv', index=False)
		>>>> quit()
		ls
		```
		![300](https://i.imgur.com/Xv1rumC.png)
		- csv 파일 및 parquet 파일 생성 확인
- # pyspark 기초
	- ## csv 파일 불러오기
		```
		python
		>>>> from pyspark.sql import SparkSession
		>>>> spark = SparkSession.builder.appName("CSVReader").getOrCreate()
		```
		![](https://i.imgur.com/XxzurvD.png)
		- 하둡이 없다는 경고, 하둡 안쓸것이므로 무시
		- ### pyspark.sql
			- 데이터프레임(DataFrame)과 SQL을 사용해 데이터를 조작, 분석하는 기능 제공
			- #### SparkSession
				- Spark에서 사용되는 진입점(entry point)
				- Spark 애플리케이션과 클러스터 간의 연결을 담당
				- SparkSession 을 활용하면 Spark SQL, Dataframe, RDD(Resilient Distributed Dataset) API 사용 가능
				- 다양한 데이터 소스(JDBC, Hive, Parquet, JSON 등)에 연결하여 데이터 조작 가능
				- 실행 환경 설정
			- `spark = SparkSession.builder.appName("CSVReader").getOrCreate()`
				- builder : SparkSession을 생성하기 위한 빌더 객체 변환
				- appName : Spark 애플리케이션의 이름 지정
				- getOrCreate : 객체를 생성하거나 기존 객체를 반환하는 메서드
		```
		>>>> df = spark.read.option("header","true").csv("/home/ubuntu/work/spark/iris.csv")
		>>>> df.show(5)
		```
		![300](https://i.imgur.com/GOKszaE.png)
	- ## parquet 파일 불러오기
		```
		python
		>>>> from pyspark.sql import SparkSession
		>>>> spark = SparkSession.builder.appName("ParquetReader").getOrCreate()
		>>>> df = spark.read.parquet("/home/ubuntu/work/spark/iris.parquet")
		>>>> df.show(8)
		```
		![300](https://i.imgur.com/adVtpYp.png)
		- show : DataFrame 데이터를 콘솔로 출력할때 사용
	- ## 데이터스키마 확인
		```
		>>> df.printSchema()  
		>>> type(df)
		```
	- ## 전체 데이터 행(row) 개수 & 열(column) 개수 확인
		```
		>>> df.count()
		>>> df.columns
		>>> len(df.columns)
		```
		![300](https://i.imgur.com/soEVnKp.png)
	- ## 특정 행 출력
		```
		>>> df.collect()[3]
		>>> df.collect()[3:6]
		```
		![400](https://i.imgur.com/OZV6Vif.png)
		- ### collect 함수
			- DataFrame의 모든 데이터를 드라이버 프로그램으로 수집하여 반환하는 함수
			- 클러스터 내의 모든 노드에서 데이터를 모아 로콜 머신의 메모리로 가져오는데 사용
			- 따라서 너무 큰 데이터셋에 대해 사용하면 메모리 부족 문제 발생할 수도 있음
	- ## 특정 열 출력
		```
		>>> df.select('sepal_length','petal_width').show(3)
		```
		![300](https://i.imgur.com/kwV1uL6.png)
		- 데이터 프레임의 특정 열을 선택 하는데 사용
		- SQL의 SELECT 문과 유사하게 동작
		```>>> cols01 = df.select('sepal_length','petal_width')
		>>> cols01.show(5)
		```
		![300](https://i.imgur.com/KpzXNuH.png)
