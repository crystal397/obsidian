- # pyenv
	- ## net-tools 설치
		`sudo apt-get update`
		`sudo apt-get install net-tools`
	- ## pyenv 설치
		```
		sudo apt-get update; sudo apt-get install make build-essential libssl-dev zlib1g-dev \
		libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \
		libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev
		```
		`curl https://pyenv.run | bash`
		- ### pyenv 환경 설정
			```
			vim .bashrc
			
			# 가장 아래에 추가
			export PYENV_ROOT="$HOME/.pyenv"
			[[ -d $PYENV_ROOT/bin ]] && export PATH="$PYENV_ROOT/bin:$PATH"
			eval "$(pyenv init -)"
			eval "$(pyenv virtualenv-init -)"
			```
			- #### 쉘 재시작
				`exec $SHELL`
			- #### 설치 확인
				`pyenv`
			- #### 가상 환경 생성
				`pyenv install --list`
				`pyenv install 3.11.9`
			- #### 가상 환경 생성
				`pyenv versions`
				`pyenv virtualevn 3.11.9 py3_11_9`
				`pyenv activate py3_11_9`
				`pyenv deactivate`
			- #### 필요한 라이브러리 설치
				`pyenv activate py3_11_9`
				`pip install numpy`
				`pip install pandas`
				`pip install pyarrow`
				`pip install scikit-learn`
				`pip install flask`
				`pip install gunicorn`
				`pip install psycopg2-binary`
				`pyenv deactivate`
- # PySpark 라이브러리 설치
	- ## Java 설치
		![400](https://i.imgur.com/B8yIit8.png)
		`sudo apt install openjdk-17-jre-headless`
		![400](https://i.imgur.com/6YIcPH3.png)
		`java -version`
	- ## Java 환경 변수 추가
		`ls /usr/lib/jvm/`
		`vim .bashrc`
		`export JAVA_HOME=/usr/lib/jvm/java-1.17.0-openjdk-amd64`
		`source .bashrc`
		`exec $SHELL`
		`$JAVA_HOME`
	- ## pyspark 설치
		`pyenv activate py3_11_9`
		`pip install pyspark`
		`pyenv deactivate`
- #  csv -> parquet 변환
	- ## 실습 디렉토리 생성
		```
		mkdir work
		cd work
		mkdir spark
		cd spark
		
		/home/ubuntu/work/spark
		```
	- ## pandas로 csv 파일 불러오기
		```
		pyenv activate py3_11_9
		python
		>>>> import pandas as pd
		>>>> url = 'https://raw.githubusercontent.com/losskatsu/data-example/main/data/iris.csv'
		>>>> df = pd.read_csv(url)
		>>>> df.head(3)
		```
		![400](https://i.imgur.com/x1pItRJ.png)
	- ## pandas dataframe -> csv, parquet 파일 변환
		```
		>>>> df.to_parquet('/home/ubuntu/work/spark/iris.parquet', index=False)
		>>>> df.to_csv('/home/ubuntu/work/spark/iris.csv', index=False)
		>>>> quit()
		ls
		```
		![300](https://i.imgur.com/Xv1rumC.png)
		- csv 파일 및 parquet 파일 생성 확인
- # pyspark 기초
	- ## csv 파일 불러오기
		```
		python
		>>>> from pyspark.sql import SparkSession
		>>>> spark = SparkSession.builder.appName("CSVReader").getOrCreate()
		```
		![](https://i.imgur.com/XxzurvD.png)
		- 하둡이 없다는 경고, 하둡 안쓸것이므로 무시
		- ### pyspark.sql
			- 데이터프레임(DataFrame)과 SQL을 사용해 데이터를 조작, 분석하는 기능 제공
			- #### SparkSession
				- Spark에서 사용되는 진입점(entry point)
				- Spark 애플리케이션과 클러스터 간의 연결을 담당
				- SparkSession 을 활용하면 Spark SQL, Dataframe, RDD(Resilient Distributed Dataset) API 사용 가능
				- 다양한 데이터 소스(JDBC, Hive, Parquet, JSON 등)에 연결하여 데이터 조작 가능
				- 실행 환경 설정
			- `spark = SparkSession.builder.appName("CSVReader").getOrCreate()`
				- builder : SparkSession을 생성하기 위한 빌더 객체 변환
				- appName : Spark 애플리케이션의 이름 지정
				- getOrCreate : 객체를 생성하거나 기존 객체를 반환하는 메서드
		```
		>>>> df = spark.read.option("header","true").csv("/home/ubuntu/work/spark/iris.csv")
		>>>> df.show(5)
		```
		![300](https://i.imgur.com/GOKszaE.png)
	- ## parquet 파일 불러오기
		```
		python
		>>>> from pyspark.sql import SparkSession
		>>>> spark = SparkSession.builder.appName("ParquetReader").getOrCreate()
		>>>> df = spark.read.parquet("/home/ubuntu/work/spark/iris.parquet")
		>>>> df.show(8)
		```
		![300](https://i.imgur.com/adVtpYp.png)
		- show : DataFrame 데이터를 콘솔로 출력할때 사용
	- ## 데이터스키마 확인
		```
		>>> df.printSchema()  
		>>> type(df)
		```
	- ## 전체 데이터 행(row) 개수 & 열(column) 개수 확인
		```
		>>> df.count()
		>>> df.columns
		>>> len(df.columns)
		```
		![300](https://i.imgur.com/soEVnKp.png)
	- ## 특정 행 출력
		```
		>>> df.collect()[3]
		>>> df.collect()[3:6]
		```
		![400](https://i.imgur.com/OZV6Vif.png)
		- ### collect 함수
			- DataFrame의 모든 데이터를 드라이버 프로그램으로 수집하여 반환하는 함수
			- 클러스터 내의 모든 노드에서 데이터를 모아 로콜 머신의 메모리로 가져오는데 사용
			- 따라서 너무 큰 데이터셋에 대해 사용하면 메모리 부족 문제 발생할 수도 있음
	- ## 특정 열 출력
		```
		>>> df.select('sepal_length','petal_width').show(3)
		```
		![300](https://i.imgur.com/kwV1uL6.png)
		- 데이터 프레임의 특정 열을 선택 하는데 사용
		- SQL의 SELECT 문과 유사하게 동작
		```>>> cols01 = df.select('sepal_length','petal_width')
		>>> cols01.show(5)
		```
		![300](https://i.imgur.com/KpzXNuH.png)
- ### day 02
- #### 복습
	```
	# ec2 접속
	# pyenv 실행
	pyenv activate py3_11_9
	
	# python 실행
	$ python
	
	# parquet 파일 불러오기
	>>> from pyspark.sql import SparkSession
	>>> spark = SparkSession.builder.appName("ParquetReader").getOrCreate()
	>>> df = spark.read.parquet("/home/ubuntu/work/spark/iris.parquet")
	>>> df.show(7)
	
	# 데이터스키마 확인
	>>> df.printSchema()
	
	# 전체 데이터 행(row) 개수 & 열(column) 개수 확인
	>>> df.count()
	>>> df.columns
	>>> len(df.columns)
	
	# 특정 행 출력
	>>> df.collect()[3]
	>>> df.collect()[3:6]
	
	# 특정 열 출력
	>>> df.select('sepal_length','sepal_width').show(3)
	>>> cols = df.select('sepal_length','sepal_width')
	>>> cols.show(3)
	```
	- ## 기초통계량 출력
		`df.describe().show()`
		![600](https://i.imgur.com/4RbPOt6.png)
		- stddev : 표준편차
		- 평균(location parameter) -> 데이터의 위치를 알수 있다
		- 표준편차(scale parameter) -> 데이터의 흩어짐 정도
	- ## filter 기능
		`>>> df.filter("sepal_length > 7").show(3)`
		![300](https://i.imgur.com/GFYlcbK.png)
		- filter : 조건에 맞는 행들만 선택
	- ## filter & select
		`>>> df.filter("sepal_length > 7").select("sepal_length", "petal_length").show(10)`
		![400](https://i.imgur.com/OD39cU8.png)
		- sepal_length가 7보다 큰 데이터 중 sepal_length, petal_length 열 만 보여줌
	- ## group by
		`>>> df.groupby("class").agg(avg("sepal_length")).show()`
		`>>> mean_sl = df.groupby("class").agg(avg("sepal_length"))`
		`>>> mean_sl.show()`
		- 클래스 별로 묶어서 클래스 별로 평균
		![300](https://i.imgur.com/8qGdWYZ.png)
		![300](https://i.imgur.com/XJM6YvN.png)
	- ## order by(오름차순)
		`df.orderBy("petal_length", ascending=True).show(5)`
		![300](https://i.imgur.com/OYg8Esq.png)
	- ## order by(내림차순)
		`df.orderBy("petal_length", ascending=True).show(5)`
		![300](https://i.imgur.com/R9BfviY.png)
		```
		>>> quit()
		$ pyenv deactivate
		```
- # Spark
	![400](https://i.imgur.com/ILeUA0E.png)
	- 지금까지는 스파크 라이브러리 를 설치하고 사용함
	- 주피터 랩을 사용해서 parquet 파일 불러오기
	- ### 디렉토리 정리
		```
		$ cd work
		
		/work$ mv spark spark01
		
		/work/spark01$ mkdir data
		
		/work/spark01$ mv iris.csv iris.parquet data
		
		/work/spark01$ cd data
		
		/work/spark01/data$ ls
		iris.csv  iris.parquet
		
		/work/spark01/data$ pwd
		/home/ubuntu/work/spark01/data
		
		cd 
		$ mkdir app
		$ cd app
		/app$ mkdir spark
		/app$ cd spark
		```
	- ## spark download
		- https://www.apache.org/dyn/closer.lua/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
			![400](https://i.imgur.com/uOVKsIc.png)
		`/app/spark$ wget https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz`
	- ## 압축 풀기
		`tar -zxvf spar-3.5.1-bin-hadoop3.tgz`
	- ## .bashrc 설정 추가
		```
		vim .bashrc
		
		export SPARK_HOME=/home/ubuntu/app/spark/spark-3.5.1-bin-hadoop3
		export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
		export PYSPARK_PYTHON=/home/ubuntu/.pyenv/versions/py3_11_9/bin/python
		
		source .bashrc
		exec $SHELL
		```
		`/app/spark/spark-3.5.1-bin-hadoop3$ ls`
		![](https://i.imgur.com/NWMBFUJ.png)
		- bin : 스파크와 상호작용할 수 있는 스크립트로 구성
		- kubernetes : 쿠버네티스 클러스터에서 쓰는 스파크를 위한 도커 이미지 제작을 위한 Dokerfile 포함
		- python : pyspark과 관련된 파이썬 파일 및 라이브러리 포함 (스파크를 실행할 때는 로컬에 설치된 파이썬 사용)
		- sbin : 스파크 컴포넌트 시작 및 중지, 관리
		`/app/spark/spark-3.5.1-bin-hadoop3/bin$ ls`
		 ![600](https://i.imgur.com/bggHoi2.png)
		 - pyspark : 파이썬으로 실행
		 - spark-shell : 스칼라로 실행
	 - ## spark 실행
		 `./pyspark`
		 ![600](https://i.imgur.com/mUrulB7.png)
		 ```
		 # 4040 포트로 web ui 확인 가능
		 http://ip-172-31-7-51.ap-northeast-2.compute.internal:4040
		 ```
		 - {ec2 퍼블릭 IP}:4040
		 ![400](https://i.imgur.com/NeBGSwR.png)
		```
		 # 스파크 세션이 이미 만들어져 있음
		 `SparkSession available as 'spark'.`
		```
		`df = spark.read.parquet("/home/ubuntu/work/spark01/data/iris.parquet")`
		`df.show(7)`
		![300](https://i.imgur.com/ed6ULUO.png)
	- ## spark web ui 확인
		- 4040포트 열어주기
		![](https://i.imgur.com/8fPcaL1.png)
		![](https://i.imgur.com/ic9GgAn.png)
		- ### spark 종료
			`>>> quit()`
- # Jupyter Lab 설치
	- ## Jupyter Lab 디렉토리 설정
		```
		cd
		cd work
		mkdir jupyter
		cd jupyter
		pwd
		```
	- ## Jupyter Lab 설치
		```
		pyenv activate py3_11_9
		pip install hupyterlab
		```
	- ## Jupyter Lab 설정
		`jupyter lab --generate-config`
		![400](https://i.imgur.com/3gfqmXO.png)
		![400](https://i.imgur.com/2yrgxg4.png)
		`vim jupyter_lab_config.py`
		```
		# 모든 네트워크 요청 수신
		c.NotebookApp.ip = '0.0.0.0' 
		# 자동으로 웹 브라우저를 열지 않음
		c.NotebookApp.open_browser = False 
		# 주피터 랩 서버가 사용할 포트
		c.NotebookApp.port = 8888 
		# 주피터 랩 서버에 접속할 때 사용할 인증 토큰, 빈 문자열로 설정하면 토큰 인증 비활성화
		c.NotebookApp.token = '' 
		# 주피터 랩 서버에 접속할 때 사용할 인증 비밀번호, 빈 문자열로 설정하면 비밀번호 비활성화
		c.NotebookApp.password = ''
		# 주피터 랩의 루트 디렉토리
		c.ServerApp.root_dir = '/home/ubuntu/work/jupyter'
		```
		![300](https://i.imgur.com/C97qwDo.png)
	- ## .bashrc 설정 추가
		```
		cd
		vim .bashrc
		
		export PYSPARK_DRIVER_PYTHON="jupyter"
		export PYSPARK_DRIVER_PYTHON_OPTS="lab"
		```
		- ### 셀 재시작
			`pyenv deactivate`
			`source .bashrc`
			`exec $SHELL`
	- ## findspark 설치
		```
		pyenv activate py3_11_9
		pip install findspark
		```
	- ## AWS 보안 설정
		![600](https://i.imgur.com/7oVC2ZH.png)
		- 8888포트 열어주기
- # Jupyter Lab 실행
	- ## Jupyter Lab 실행
		`$ jupyter lab`
		{ec2 퍼블릭 IP}:8888
		![300](https://i.imgur.com/ODxEfHY.png)
		- Notebook -> Python 3 클릭
		![300](https://i.imgur.com/klb4qIF.png)
		- ### 단축기 +
			- rename
			![200](https://i.imgur.com/seeSvAb.png)
			- ctrl + s  누르면 저장(로그 출력됨)
				![400](https://i.imgur.com/87XzHji.png)
			- shift + s 라인 실행
			- 위 라인 추가, 아래 라인 추가, 삭제
				![100](https://i.imgur.com/h7lvMdS.png)
			- a : 위 추가
			- b : 아래 추가
			- dd : 삭제
			- z : 이전으로
			- 커널 초기화
				![300](https://i.imgur.com/OGdNVP7.png)
			- 커널 초기화 + 전부 실행
				![200](https://i.imgur.com/V8DDebn.png)
			- 마크다운
				![200](https://i.imgur.com/E898lhW.png)
				- 작성 후 shift + Enter
				- 수정 은 더블클릭
				- 예시
					```md
					# 월간 보고서
					## 주간 보고서
					### 마크다운 문법(markdown)
					* 사과
					* 배
					* 포도
					이름 | 소속 | 취미
					-----|-----|-----
					문기오 | 이어드림 4기 | 게임
					강수빈 | 이어드림 4기 | 코딩
					```
					![300](https://i.imgur.com/UhELjtD.png)
	- ### 실습
		`import findspark`
		`findspark.init("/home/ubuntu/app/spark/spark-3.5.1-bin-hadoop3")`
		- ↑ 주피터 랩 과 스파크 를 붙임
		`spark = SparkSession.builder.appName("test").getOrCreate()`
		`df = spark.read.parquet("/home/ubuntu/work/spark01/data/iris.parquet")`
		`df.show(3)`
			![300](https://i.imgur.com/EUogNd5.png)
	- 주피터 노트북 확장자로 .ipynb 많이 사용함
- # Spark cluster 구축
	![400](https://i.imgur.com/mfTFQNX.png)
	- 서버 1을 마스터로
	- 나머지는 worker
	- ## 두 번째, 세번째 인스턴스 생성
		- 인스턴스 이름 : 이름02, 이름03
		![200](https://i.imgur.com/X9ssmio.png)
		- ubuntu 선택
		![200](https://i.imgur.com/jMBvntH.png)
		- t2.xlage 선택
		![200](https://i.imgur.com/mVPWTnd.png)
		- 01 과 같은 보안그룹 선택
		![200](https://i.imgur.com/AUyaASc.png)
		- 스토리지 구성 100GiB
		- 인스턴스 시작
		- ### 01 ~ 03 전부 접속 확인
			`ssh -i ~/.ssh/mungio.pem ubuntu@{퍼블릭 IP}`
			`df -h` -> 용량 확인
- # 인스턴스 간 트래픽 허용
	- ## 인스턴스간 ping 가능하게 설정
		![200](https://i.imgur.com/Eup4PHG.png)
		- 같은 보안그룹으로 만든지 확인!!
		![500](https://i.imgur.com/oTzBl9T.png)
		- 규칙 추가 -> 모든 트래픽 -> 본인 사용한 보안그룹 검색해서 선택
	- ## net-tools 설치
		- 02, 03 둘다 실행
			`sudo apt update`
			`sudo apt install net-tools`
			- 01은 이미 설치 되어 있음
	- ## ip 주소 확인
		`ifconfig`
		`sudo vim /etc/hosts`
		- 3개 전부 작성 해주기
		```
		{IP 주소} {호스트 이름}
		172.31.7.51 ip-172-31-7-51
		172.31.3.190 ip-172-31-3-190
		172.31.6.106 ip-172-31-6-106
		```
- # ssh 인증키 생성
	- ssh란 ?
		- Secure Shell Protocol
		- 네트워크 프로토콜 중 하나
		- Public Network를 통해 서로 통신을 할 때 보안적으로 안전하게 통신을 하기 위해 사용하는 프로토콜이다
	- ## ssh-keygen -t rsa
		- RSA 알고리즘을 사용해 SSH 키 쌍을 생성하는 명령어
		- 명령어 입력 후에는 해당 경로에 다음과 같은 두 개의 파일이 생성됨
			- id_rsa : 개인 키(private key) 파일
			- id_rsa.pub : 공개 키(public key) 파일
	- ## ssh 인증키 생성
		- 서버 3개 전부 실행
		![400](https://i.imgur.com/jUP5cL0.png)
		- 명령어 입력후 Enter 3번 입력
	- ## ssh 인증키 생성 확인
		![400](https://i.imgur.com/275tn4S.png)
		![400](https://i.imgur.com/QWtwCvq.png)
		![400](https://i.imgur.com/qDN7byp.png)


