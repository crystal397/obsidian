- # Python 오퍼레이터란?
	- 파이썬 함수를 실행시킬 수 있는 오퍼레이터
	- 가장 많이 사용되는 오퍼레이터중 하나
		- https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/python.html
	- dags_python_operator
		```
		from airflow import DAG
		import pendulum
		from airflow.operators.python import PythonOperator
		import random
		
		with DAG(
			dag_id="dags_python_operator",
			schedule="30 6 * * *",
			start_date=pendulum.datetime(2024, 6, 16, tz="Asia/Seoul"),
			catchup=False
		) as dag:
		def select_fruit():
			fruit = ['APPLE','BANANA','ORANGE','AVOCADO']
			rand_int = random.randint(0,3)
			print(fruit[rand_int])
		
		py_t1 = PythonOperator(
			task_id='py_t1',
			python_callable=select_fruit
		)
		
		py_t1
		```
		- 외부의 파이썬 함수 실행시키기
			- 일반적으로 파이썬 함수는 dag 내부에 선언해놓기 보다
			- 공통 모듈로써 활용할 수 있도록 만드는 경우가 대부분
			- dag 외부에 존재하는 함수를 실행할 수 있어야 함
		- dag 외부에서 작성해놓은 파이썬 함수는 어떻게 import 할까?
			- `from airflow.operators.python import PythonOperator`
		- 파이썬 내 Import 구문(ex. _from airflow import DAG_)를 어떻게 찾을까?
								(=airflow 파일에서 DAG 클래스를 가져와라)
		- 이것을 이해하기 위해서는 파이썬 모듈 경로를 찾는 방법 이해 필요
	- ## 파이썬 모듈 경로 이해
		- 파이썬은 sys.path 항목에 정의되어 있는 경로를 기본 경로로 인식
			```
			# 로컬 
			/airflow$ python
			>>> import sys
			>>> from pprint import pprint
			>>> pprint(sys.path)
			```
			![400](https://i.imgur.com/1tj3IGD.png)
		- dag 외부에 작성해놓은 파이썬 모듈을 인식하려면 해당 경로가 sys.path 에 존재해야함
		- sys.path에 경로를 추가하는 방법 2가지
			1) sys.path 에 명시적으로 추가(sys.path는 list 이므로 append 명령으로 추가)
			2) OS의 PYTHONPATH 에 추가
		- Airflow는 airflow 시작시 자동적으로 sys.path에 3가지 경로를 추가됨
			- (Scheduler 컨테이너에 접속하여 airflow info 명령 수행)
				- `ubuntu~$ sudo docker ps`
				- `ubuntu~$ sudo docker exec -it {scheduler ID} bash`
				- `default@~: airflow info`
					![400](https://i.imgur.com/pld45V9.png)
		- dags 디렉토리를 기본적으로 sys.path에 추가해주고 있기때문에 dags 디렉토리에 dag 파일을 가져다 놓기만 해도 airflow는 DAG 인식 가능
		- config, plugins 디렉토리도 sys.path에 추가해주고 있어서 파이썬 공통 모듈이나 설정 파일은 plugins 디렉토리 또는 config 디렉토리 안에 만들어 놓으면 dag에서 import 하여 사용 가능
			![400](https://i.imgur.com/C8PLnPe.png)
	- ## 공통함수 만들기
		- plugins 디렉토리 아래 common/common_func.py 생성
			```
			def get_sftp():
			    print('sftp 작업을 시작합니다')
			
			def regist(name, sex, *args):
			    print(f'이름: {name}')
			    print(f'성별: {sex}')
			    print(f'기타옵션들: {args}')
			
			def regist2(name, sex, *args, **kwargs):
			    print(f'이름: {name}')
			    print(f'성별: {sex}')
			    print(f'기타옵션들: {args}')
			    email = kwargs['email'] or 'empty'
			    phone = kwargs['phone'] or 'empty'
			    if email:
			        print(email)
			    if phone:
			        print(phone)
			```
	- ## DAG 만들기
		- dags_python_import_func
			```
			from airflow import DAG
			import pendulum
			import datetime
			from airflow.operators.python import PythonOperator
			from common.common_func import get_sftp
			
			with DAG(
				dag_id="dags_python_import_func",
				schedule="30 6 * * *",
				start_date=pendulum.datetime(2023, 3, 1, tz="Asia/Seoul"),
				catchup=False
			) as dag:
			
			task_get_sftp = PythonOperator(
				task_id='task_get_sftp',
				python_callable=get_sftp
			)
			```
	- ## Vscode 경로 설정
		- 로컬의 .env 파일에 PYTHONPATH 값 추가 (.env 없으면 생성)
			PYTHONPATH='{Airflow 경로}/plugins'
		- Vscode를 위한것이며 EC2 서버에 반영할 필요가 없음
		- .env 수정 반영은 필요 없으므로 .env 파일은 .gitignore에 추가하여 git push되지 않도록 관리