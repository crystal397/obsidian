- # Context
	- ## Context 란?
		- 특정 Task에 대한 정보(스케줄 시간, DAG명, Task명 등)를 담은 파이썬 딕셔너리
		- 앞서본 kwargs의 내용은 Airflow가 자동적으로 넣어 준 Context
		- Airflow 사용시 Context정보를 활용하는 경우가 많아 잘 이용하면 많은 도움이 됨
		- Context 종류 : https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html
	- ## Context 꺼내 쓰기
		- Python Operator, Task 데코레이터 모두 Context 정보는 kwargs에서 꺼내 쓸 수 있음
		- ### 실습
		- kwargs는 딕셔너리 이다.
		- kwargs에서 key가 data_interval_start와 data_interval_end 인 두 값을 꺼내어 출력
			- 실습 1) plugins/common/common_func.py 의 regist2 함수를 수정, dags_python_with_op_kwargs 실행
	        ```jsx
	        def get_sftp():
	            print('sftp 작업을 시작합니다')
	            
	        
	        def regist(name, sex, *args):
	            print(f'이름: {name}')
	            print(f'성별: {sex}')
	            print(f'기타옵션들: {args}')
	        
	        def regist2(name, sex, *args, **kwargs):
	            print(f'이름: {name}')
	            print(f'성별: {sex}')
	            print(f'기타옵션들: {args}')
	            email = kwargs['email'] or 'empty'
	            phone = kwargs['phone'] or 'empty'
	            if email:
	                print(email)
	            if phone:
	                print(phone)
	            # 하단 추가
	            data_interval_start = kwargs.get('data_interval_start')
	            data_interval_end = kwargs.get('data_interval_end')
	            print(data_interval_start)
	            print(data_interval_end)
	        ```
		    - 실습 2) dags_python_decorator_with_param 내 regist3 함수를 수정하고 실행
	        ```jsx
	        from airflow import DAG
	        import pendulum
	        from airflow.decorators import task
	        
	        with DAG(
	            dag_id="dags_python_decorator_with_param",
	            schedule="0 2 * * 1",
	            start_date=pendulum.datetime(2024, 6, 14, tz="Asia/Seoul"),
	            catchup=False,
	        ) as dag:
	        
	            @task(task_id="python_task_1")
	            def regist3(name, sex, *args, **kwargs):
	                print(f'이름: {name}')
	                print(f'성별: {sex}')
	                print(f'기타옵션들: {args}')
	                email = kwargs['email'] or 'empty'
	                phone = kwargs['phone'] or 'empty'
	                print(f'email: {email}')
	                print(f'phone: {phone}')
	                from pprint import pprint
	                pprint(kwargs)
	                
	                # 똑같이 추가
	                data_interval_start = kwargs.get('data_interval_start')
	                data_interval_end = kwargs.get('data_interval_end')
	                print(data_interval_start)
	                print(data_interval_end)
	                
	            python_task_1 = regist3('hjkim', 'man', 'seoul', email='hjkim_sun@naver.com', phone='010')
	        ```
- # Airflow의 날짜 개념
	- ## dat_interval_start와 data_interval_end
		- 가장 많이 사용되는 context 변수는 data_interval_start와 data_interval_end
		- 두 값이 어떤 의미를 갖는지 이해하기 위해 Airflow의 날짜 개념 이해
		    ![400](https://i.imgur.com/XLPRMVr.png)
		- 일반적으로 배치일을 기준일이라 생각하는 경우가 많으나 ==Airflow에서는 데이터 관점의 시작일을 기준으로 바라봄==
			![400](https://i.imgur.com/Y2yXkEr.png)
		- ### 날짜 개념 예시
			- data_interval_start는 배치 시점, data_interval_end는 현재 배치 시점이라 생각해도 무방
				![400(https://i.imgur.com/kHXnLvx.png)
- # Template란?
	- 문서(파일)에서 특정 양식으로 작성된 값을 런타임시 실제 값으로 치환해주는 처리 엔진	 
	- 템플릿 엔진은 여러 솔루션이 존재하므로 그중 Jinja 템플릿은 파이썬 언어에서 사용하는 엔진
		![350](https://i.imgur.com/ujaeac4.png)
		- Jinja 템플릿, 어디에 쓰이나?
			- 파이썬 기반 웹 프레임워크인 Flask, Django 뿐만 아니라 수만은 Application에서 적용됨
			(Flask,Django의 경우 주로 HTML 템플릿 저장 후 화면에 보여질때 실제 값으로 변환 후 출력)
			- SQL작성시에도 활용 가능
	- ## Airflow에서 Template 사용
		- 오퍼레이터 파라미터 집력시 중괄호{} 2개를 이용하면
			Airflow에서 기본적으로 제공하는 context를 실제 값으로 입력 받을 수 있음.
			- ex. data_interval_start, data_interval_end, dag명, task명 등
			```jsx
			{{data_interval_end}} -> DataTime(2024, 6, 16, 0, 0, tzinfo=Timezone('UTC'))
			{{ds}} -> 2024-06-16
			```
		- 뿐만 아니라 Template 사용시 포맷 변환과 날짜 연산이 가능
			```jsx
			{{data_interval_end|ds}} -> 2024-06-17
			{{data_interval_end|ds_nodash}} -> 20240617
			```
	- ## 오퍼레이터에서 Template 사용 하려면?
		- Bash 오퍼레이터나 Python 오퍼레이터 등 오퍼레이터의 모든 파라미터에 Template를 적용할 수 있는 것은 아님
		- 오퍼레이터 사용시 반드시 문서를 읽고 어떤 파라미터에 Template 적용이 가는한지 확인
			( Bash오퍼레이터 : https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/bash/index.html)
			( Python오퍼레이터 : https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/python/index.html)
	- ### 실습) Bash 오퍼레이터에서 Template 사용
		- dags_bash_with_template.py
			```jsx
			from airflow import DAG
			import pendulum
			from airflow.operators.bash import BashOperator
			
			with DAG(
			    dag_id="dags_bash_with_template",
			    schedule="10 0 * * *",
			    start_date=pendulum.datetime(2024, 6, 16, tz="Asia/Seoul"),
			    catchup=False
			) as dag:
			    bash_t1 = BashOperator(
			        task_id='bash_t1',
			        bash_command='echo "data_interval_end: {{ data_interval_end }}  "'
			    )
			
			    bash_t2 = BashOperator(
			        task_id='bash_t2',
			        env={
			            'START_DATE':'{{data_interval_start | ds }}',
			            'END_DATE':'{{data_interval_end | ds }}'
			        },
			        bash_command='echo $START_DATE && echo $END_DATE'
			    )
			
			    bash_t1 >> bash_t2
			```
	- ### 실습) Python 오퍼레이터에서 Template 사용
		- dags_python_template.py
			```jsx
			from airflow import DAG
			import pendulum
			from airflow.operators.python import PythonOperator
			from airflow.decorators import task
			
			with DAG(
			    dag_id="dags_python_template",
			    schedule="30 9 * * *",
			    start_date=pendulum.datetime(2024, 6, 16, tz="Asia/Seoul"),
			    catchup=False
			) as dag:
			    
			    def python_function1(start_date, end_date, **kwargs):
			        print(start_date)
			        print(end_date)
			
			    python_t1 = PythonOperator(
			        task_id='python_t1',
			        python_callable=python_function1,
			        op_kwargs={'start_date':'{{data_interval_start | ds}}', 'end_date':'{{data_interval_end | ds}}'}
			    )
			
			    @task(task_id='python_t2')
			    def python_function2(**kwargs):
			        print(kwargs)
			        print('ds:' + kwargs['ds'])
			        print('ts:' + kwargs['ts'])
			        print('data_interval_start:' + str(kwargs['data_interval_start']))
			        print('data_interval_end:' + str(kwargs['data_interval_end']))
			        print('task_instance:' + str(kwargs['ti']))
			
			
			    python_t1 >> python_function2()
			```
	- ## Bash Operator VS Python Operator
		- Python Operator는 kwargs 에서 직접 Context 변수를 꺼내 쓸 수 있다
		- Bash Operator는 직접 Context를 꺼낼 수 있는 방법이 없음
		- Bash Operator는 Context 값을 얻기 위해 Template를 사용하는 것이 필수
		- Python Operator에서는 Template 사용 or kwargs에서 직접 저근 선택이 가능함
- # Macro
	- ## Macro 란?
		- Template 안에서 파이썬 문법을 이용해 포맷 변경, 날짜 연산 등이 가능한 방법
			https://airflow.apache.org/docs/apache-airflow/stable/templates-ref.html
		![450](https://i.imgur.com/58crZjc.png)
		- 특히 날짜를 더하거나 빼는 등 연산 수행시 파이썬의 datetime, dateutil 라이브러리에 익숙해져야 함
	- ## 파이썬 datetime & dateutil
		- dateutil 라이브러리 활용시 날짜/시간을 더하거나 빼기에 수월하며 이 방법은 Airflow의 macro에도 활용됨
			![250](https://i.imgur.com/Up6UQPV.png)
	- ### 실습) Bash 오퍼레이터에서 macro 활용 1
		- 매월 말일 0 시 10분에 수행되는 DAG
			```jsx
			from airflow import DAG
			import pendulum
			from airflow.operators.bash import BashOperator
			
			with DAG(
			    dag_id="dags_bash_with_macro_eg1",
			    schedule="10 0 L * *",
			    start_date=pendulum.datetime(2024, 5, 1, tz="Asia/Seoul"),
			    catchup=False
			) as dag:
			    # START_DATE: 전월 말일, END_DATE: 1일 전
			    bash_task_1 = BashOperator(
			        task_id='bash_task_1',
			        env={'START_DATE':'{{ data_interval_start.in_timezone("Asia/Seoul") | ds }}',
			             'END_DATE':'{{ (data_interval_end.in_timezone("Asia/Seoul") - macros.dateutil.relativedelta.relativedelta(days=1)) | ds}}'
			        },
			        bash_command='echo "START_DATE: $START_DATE" && echo "END_DATE: $END_DATE"'
			    )
			```


